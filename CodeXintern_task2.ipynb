{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef60daa1-4b0c-4e99-a03b-abfecf912c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\harsh\\anaconda3\\lib\\site-packages (2.2.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\harsh\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\harsh\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "pip install pandas scikit-learn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e7a31f5-9700-4cc6-92d2-6514de20cc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1655f7b6-3f3c-4cbc-9a41-5c974fb19b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'spam.csv' not found. Please download the SMS Spam Collection Dataset and place it in the working directory.\n",
      "Dataset loaded with 5 samples.\n",
      "  label                         message\n",
      "0   ham  Go until jurong point, crazy..\n",
      "1  spam        SIX chances to win CASH!\n",
      "2   ham     Are you available tomorrow?\n",
      "3  spam      Had your mobile 11 months?\n",
      "4   ham         I love the simple life!\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load the messages and labels ---\n",
    "# The SMS Spam Collection Dataset is typically a tab-separated file.\n",
    "# We'll load it assuming the file is named 'sms_spam_collection.csv' \n",
    "# or use a common URL for demonstration if a local file isn't available.\n",
    "# NOTE: Replace 'spam.csv' with the actual path to your downloaded dataset file.\n",
    "try:\n",
    "    # Assuming a common format for the UCI SMS Spam Collection\n",
    "    df = pd.read_csv(\n",
    "        'spam.csv', \n",
    "        encoding='latin-1', \n",
    "        usecols=[0, 1], \n",
    "        names=['label', 'message'], \n",
    "        header=None, \n",
    "        sep='\\t'\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'spam.csv' not found. Please download the SMS Spam Collection Dataset and place it in the working directory.\")\n",
    "    # Create a small sample DataFrame for demonstration if the file is missing\n",
    "    data = {'label': ['ham', 'spam', 'ham', 'spam', 'ham'],\n",
    "            'message': ['Go until jurong point, crazy..', 'SIX chances to win CASH!', \n",
    "                        'Are you available tomorrow?', 'Had your mobile 11 months?', \n",
    "                        'I love the simple life!']}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "print(f\"Dataset loaded with {len(df)} samples.\")\n",
    "print(df.head())\n",
    "\n",
    "# Map 'spam' to 1 and 'ham' to 0 for numerical processing\n",
    "df['label_num'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "X = df['message']\n",
    "y = df['label_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e32811f-7d5a-461e-9672-8cb0a3e6fd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK stopwords not found. Please run: import nltk; nltk.download('stopwords')\n",
      "\n",
      "First 5 processed messages:\n",
      "0    go until jurong point crazy\n",
      "1        six chances to win cash\n",
      "2     are you available tomorrow\n",
      "3         had your mobile months\n",
      "4         i love the simple life\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Preprocess the text (Custom Preprocessing Function) ---\n",
    "\n",
    "# Get English stopwords from NLTK (requires 'nltk.download('stopwords')' once)\n",
    "try:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "except LookupError:\n",
    "    # Fallback if nltk.download('stopwords') hasn't been run\n",
    "    print(\"NLTK stopwords not found. Please run: import nltk; nltk.download('stopwords')\")\n",
    "    stop_words = set() \n",
    "\n",
    "def text_preprocess(text):\n",
    "    \"\"\"\n",
    "    Performs lowercasing, removes punctuation, tokenizes (implicitly), \n",
    "    and removes stopwords.\n",
    "    \"\"\"\n",
    "    # Lowercasing and removing punctuation\n",
    "    text = text.lower()\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "    \n",
    "    # Tokenization and removing stopwords (implicit tokenization in the next step)\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words and word.isalpha()]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply preprocessing to the messages\n",
    "X_processed = X.apply(text_preprocess)\n",
    "print(\"\\nFirst 5 processed messages:\")\n",
    "print(X_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f4412cc-174a-4af7-a6f4-647ed6578db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature matrix shape: (5, 22)\n",
      "Training set size: 3, Test set size: 2\n",
      "\n",
      "Model trained (Multinomial Naive Bayes).\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Convert text into numeric features (TF-IDF) ---\n",
    "# TF-IDF (Term Frequency-Inverse Document Frequency) is a great feature\n",
    "# extraction method that weights words by their importance.\n",
    "tfidf = TfidfVectorizer(max_features=5000) # Use the top 5000 words\n",
    "X_features = tfidf.fit_transform(X_processed)\n",
    "print(f\"\\nFeature matrix shape: {X_features.shape}\")\n",
    "\n",
    "# --- 4. Split into train/test sets ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_features, y, test_size=0.25, random_state=42\n",
    ")\n",
    "print(f\"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "# --- 5. Train a simple model (Naive Bayes) ---\n",
    "# Multinomial Naive Bayes is excellent for text classification\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"\\nModel trained (Multinomial Naive Bayes).\")\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ad602fd-a36f-41a6-a096-6fdcd8a02de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Performance Metrics ---\n",
      "Accuracy: 0.5000\n",
      "Precision: 0.0000\n",
      "Recall:    0.0000\n",
      "F1 Score:  0.0000\n",
      "---------------------------------\n",
      "\n",
      "Custom Test Cases:\n",
      "Message 1: 'WINNER! You have won a free holiday. Call now!' -> HAM\n",
      "Message 2: 'Hey, are we still meeting for lunch today?' -> HAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HARSH\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Measure performance ---\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n--- Model Performance Metrics ---\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# Optional: Test with custom messages\n",
    "def predict_message(message):\n",
    "    processed = text_preprocess(message)\n",
    "    features = tfidf.transform([processed])\n",
    "    prediction = model.predict(features)[0]\n",
    "    return \"SPAM\" if prediction == 1 else \"HAM\"\n",
    "\n",
    "print(\"\\nCustom Test Cases:\")\n",
    "print(f\"Message 1: 'WINNER! You have won a free holiday. Call now!' -> {predict_message('WINNER! You have won a free holiday. Call now!')}\")\n",
    "print(f\"Message 2: 'Hey, are we still meeting for lunch today?' -> {predict_message('Hey, are we still meeting for lunch today?')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638565ed-db3e-4fbf-84f2-43a6d275b1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
